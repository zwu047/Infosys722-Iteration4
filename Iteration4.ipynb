{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "# findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "findspark.init('/home/ubuntu/spark-3.1.1-bin-hadoop3.2')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Iteration4').getOrCreate()\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the datasets\n",
    "dataset_one = spark.read.csv('./student-math-dataset1.csv', header=True, inferSchema=True)\n",
    "dataset_two = spark.read.csv('./student-math-dataset2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Describe the data\n",
    "print(\"Dataset one structure: \" + str(dataset_one.count()) + \" rows, and \" + str(len(dataset_one.columns)) + \" columns\")\n",
    "print(\"Dataset two structure: \" + str(dataset_two.count()) + \" rows, and \" + str(len(dataset_two.columns)) + \" columns\")\n",
    "print(\"How many students inside the dataset: \" + str(dataset_one.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data type inside the dataset one\")\n",
    "dataset_one.dtypes\n",
    "# print(dataset_one.printSchema())\n",
    "# print(dataset_two.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data type inside the dataset two\")\n",
    "dataset_two.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Explore the data\n",
    "# need to see the difference between .show() vs .describe().shows()\n",
    "print(\"Dataset one information: \")\n",
    "dataset_one.describe().show() # truncate=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_two.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above\n",
    "# dataset_one.select(dataset_one.columns).describe().show()\n",
    "# dataset_two.select('G1', 'G2', 'G3').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student first year grade distribution: \")\n",
    "plt.xlabel('First Year Grade', fontsize = 10)\n",
    "plt.ylabel('Number of students', fontsize = 10)\n",
    "plt.title('Student first year grade distribution', fontsize = 15)\n",
    "diagram_one = dataset_two.toPandas()['G1'].value_counts()\n",
    "plt.bar(diagram_one.index, diagram_one.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student second year grade distribution: \")\n",
    "plt.xlabel('Second year Grade', fontsize = 10)\n",
    "plt.ylabel('Number of students', fontsize = 10)\n",
    "plt.title('Student second year grade distribution', fontsize = 15)\n",
    "diagram_two = dataset_two.toPandas()['G2'].value_counts()\n",
    "plt.bar(diagram_two.index, diagram_two.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student final year grade distribution: \")\n",
    "plt.xlabel('Final year Grade', fontsize = 10)\n",
    "plt.ylabel('Number of students', fontsize = 10)\n",
    "plt.title('Student final year grade distribution', fontsize = 15)\n",
    "diagram_three = dataset_two.toPandas()['G3'].value_counts()\n",
    "plt.bar(diagram_three.index, diagram_three.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gender distribution inside the dataset: \")\n",
    "plt.xlabel('Gender', fontsize = 10)\n",
    "plt.ylabel('Number of students', fontsize = 10)\n",
    "plt.title('Gender distribution', fontsize = 15)\n",
    "diagram_four = dataset_one.toPandas()['sex'].value_counts()\n",
    "plt.bar(diagram_four.index, diagram_four.values)\n",
    "plt.show()\n",
    "\n",
    "print('Number of male students:', len(dataset_one.toPandas()[dataset_one.toPandas()['sex'] == 'M']))\n",
    "print('Number of female students:', len(dataset_one.toPandas()[dataset_one.toPandas()['sex'] == 'F']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student age distribution: \")\n",
    "plt.xlabel('Age', fontsize = 10)\n",
    "plt.ylabel('Number of students', fontsize = 10)\n",
    "plt.title('Student age distribution', fontsize = 15)\n",
    "diagram_five = dataset_one.toPandas()['age'].value_counts()\n",
    "plt.bar(diagram_five.index, diagram_five.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student in Urban or rural areas distribution\")\n",
    "plt.xlabel('Area', fontsize = 10)\n",
    "plt.ylabel('Number of students', fontsize = 10)\n",
    "plt.title('Student in Urban or rural areas distribution', fontsize = 15)\n",
    "diagram_six = dataset_one.toPandas()['address'].value_counts()\n",
    "plt.bar(diagram_six.index, diagram_six.values)\n",
    "plt.show()\n",
    "\n",
    "print('Number of students in urban area:', len(dataset_one.toPandas()[dataset_one.toPandas()['address'] == 'U']))\n",
    "print('Number of students in rural area:', len(dataset_one.toPandas()[dataset_one.toPandas()['address'] == 'R']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "# 2.4 Verify the data quality\n",
    "# missing value from freetime, dalc, walc, health field.\n",
    "print(\"Missing value or null value in dataset one: \")\n",
    "print(dataset_one.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataset_one.columns]).toPandas().transpose())\n",
    "print()\n",
    "print(\"Missing value or null value in dataset two: \")\n",
    "print(dataset_two.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataset_two.columns]).toPandas().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 integraet various data source\n",
    "# combine the dataset\n",
    "dataset_three = dataset_one.join(dataset_two, on=['id'])\n",
    "print(\"Dataset one structure: \" + str(dataset_three.count()) + \" rows, and \" + str(len(dataset_three.columns)) + \" columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Explore the dataï¼ˆmore ploting)\n",
    "# plot more diagram in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Students go out time: \")\n",
    "diagram_seven_x = dataset_three.toPandas()['goout']\n",
    "diagram_seven_y = dataset_three.toPandas()['G3']\n",
    "plt.xlabel('Go out', fontsize = 10)\n",
    "plt.ylabel('Final Grade G3', fontsize = 10)\n",
    "plt.title('Students go out time', fontsize = 15)\n",
    "plt.scatter(diagram_seven_x, diagram_seven_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Students study time: \")\n",
    "diagram_eight_x = dataset_three.toPandas()['studytime']\n",
    "diagram_eight_y = dataset_three.toPandas()['G3']\n",
    "plt.xlabel('Study Time', fontsize = 10)\n",
    "plt.ylabel('Final Grade G3', fontsize = 10)\n",
    "plt.title('Students study time', fontsize = 15)\n",
    "plt.scatter(diagram_eight_x, diagram_eight_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mother's job:\")\n",
    "plt.xlabel(\"Mother's job\", fontsize = 10)\n",
    "plt.ylabel('Counts', fontsize = 10)\n",
    "plt.title(\"Mother's job\", fontsize = 15)\n",
    "diagram_nine = dataset_three.toPandas()[\"Mjob\"].value_counts()\n",
    "plt.bar(diagram_nine.index, diagram_nine.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Father's job:\")\n",
    "plt.xlabel(\"Father's job\", fontsize = 10)\n",
    "plt.ylabel('Counts', fontsize = 10)\n",
    "plt.title(\"Mother's job\", fontsize = 15)\n",
    "diagram_ten = dataset_three.toPandas()[\"Fjob\"].value_counts()\n",
    "plt.bar(diagram_ten.index, diagram_ten.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Father's education level:\")\n",
    "plt.xlabel(\"Father's education level\", fontsize = 10)\n",
    "plt.ylabel('Counts', fontsize = 10)\n",
    "plt.title(\"Father's education level\", fontsize = 15)\n",
    "diagram_eleven = dataset_three.toPandas()[\"Fedu\"].value_counts()\n",
    "plt.bar(diagram_eleven.index, diagram_eleven.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mother's education level:\")\n",
    "plt.xlabel(\"Mother's education level\", fontsize = 10)\n",
    "plt.ylabel('Final Grade G3', fontsize = 10)\n",
    "plt.title(\"Mother's education level\", fontsize = 15)\n",
    "diagram_twelve = dataset_three.toPandas()[\"Medu\"].value_counts()\n",
    "plt.bar(diagram_twelve.index, diagram_twelve.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Clean the data\n",
    "# removing the missing value\n",
    "print(\"Dataset three length before removing the missing value:\", dataset_three.count())\n",
    "dataset_three = dataset_three.na.drop()\n",
    "print(\"Dataset three length after removing the missing value:\", dataset_three.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Format the data as required\n",
    "print(\"Data type inside the dataset three before formating: \")\n",
    "dataset_three.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "# indexer = StringIndexer(inputCol=\"Mjob\", outputCol=\"Mjob_index\")\n",
    "# dataset_three = indexer.fit(dataset_three).transform(dataset_three)\n",
    "# dataset_three = dataset_three.drop('Mjob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = dict(dataset_three.dtypes)\n",
    "column_names = []\n",
    "for key, value in types.items():\n",
    "    if value == 'string':\n",
    "        column_names.append(key)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "for item in column_names:\n",
    "    indexer = StringIndexer(inputCol=str(item), outputCol= str(item) + '_index')\n",
    "    dataset_three = indexer.fit(dataset_three).transform(dataset_three)\n",
    "for item in column_names:\n",
    "    dataset_three = dataset_three.drop(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_three.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Clean the data (part two)\n",
    "# removing the outliers value\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "# print(\"Dataset three length before removing the outliers value:\", dataset_three.count())\n",
    "# dataset_three_z = np.abs(stats.zscore(dataset_three))\n",
    "# threshold = 5\n",
    "# dataset_three = dataset_three[(dataset_three_z <= threshold).all(axis=1)]\n",
    "# print(\"Dataset three length after removing the outliers value:\", dataset_three.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Reduce the data\n",
    "# Find correlations with the Grade\n",
    "dataset_three_correlated_fileds = dataset_three.corr()['G3'].sort_values(ascending=False)\n",
    "print(\"List out the top 30 fields which has strong correlations relationship with the Final Grade G3: \")\n",
    "print(dataset_three_correlated_fileds[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"G2\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"G3\")\n",
    "result = selector.fit(dataset_three).transform(dataset_three)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Project the data\n",
    "# Removed\n",
    "# Will only grabe the top 10 most correlation features for the further study\n",
    "dataset_three_correlated_fileds = dataset_three_correlated_fileds[:10]\n",
    "print(dataset_three_correlated_fileds)\n",
    "\n",
    "dataset_three = dataset_three.loc[:, dataset_three_correlated_fileds.index]\n",
    "print(\"\\nThe final dataset strucure after cleaning and data transformation:\", dataset_three.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_three = dataset_one.join(dataset_two, on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_three.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import ChiSqSelector\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# df = spark.createDataFrame([\n",
    "#     (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "#     (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "#     (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "# selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "#                          outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "# result = selector.fit(df).transform(df)\n",
    "\n",
    "# print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "# result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import PCA\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "#         (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "#         (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "# df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "# model = pca.fit(df)\n",
    "\n",
    "# result = model.transform(df).select(\"pcaFeatures\")\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
